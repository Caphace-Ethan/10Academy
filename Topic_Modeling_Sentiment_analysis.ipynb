{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ce70a0",
   "metadata": {},
   "source": [
    "# Project: Topic Modeling and Sentiment Analysis on Twitter Data\n",
    "\n",
    "## **Objective **\n",
    "### Social Media Tweet Analysis on Twitter Dataset\n",
    "*   Topic Modeling on Twitter Dataset\n",
    "*   Sentiment analysis on Twitter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618f3bb",
   "metadata": {},
   "source": [
    "### **Topic modeling**\n",
    "Topic modeling is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of texts.\n",
    "\n",
    "\n",
    "*   The task here is to discover abstract topics from tweets.\n",
    "\n",
    "\n",
    "### **Sentiment analysis**\n",
    " It is used in social media monitoring, allowing businesses to gain insights about how customers feel about certain topics, and detect urgent issues in real time before they spiral out of control.\n",
    "\n",
    "\n",
    "*   The task here is to classify a tweet as a positive or negative tweet sentiment wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3dd75e",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "### Loading necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d83f5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0e16fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_file: str)->list:\n",
    "    \"\"\"\n",
    "    json file reader to open and read json files into a list\n",
    "    Args:\n",
    "    -----\n",
    "    json_file: str - path of a json file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    length of the json file and a list of json\n",
    "    \"\"\"\n",
    "    \n",
    "    tweets_data = []\n",
    "    for tweets in open(json_file,'r'):\n",
    "        tweets_data.append(json.loads(tweets))\n",
    "    \n",
    "    return len(tweets_data), tweets_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "78feef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Successfully Saved.!!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu Jun 17 16:18:28 +0000 2021</td>\n",
       "      <td>1405947374003015684</td>\n",
       "      <td>ðŸš¨Africa is \"in the midst of a full-blown third...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri Jun 18 16:40:24 +0000 2021</td>\n",
       "      <td>1405947412364075010</td>\n",
       "      <td>Dr Moeti is head of WHO in Africa, and one of ...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fri Jun 18 17:45:27 +0000 2021</td>\n",
       "      <td>1405947447797587969</td>\n",
       "      <td>Thank you @research2note for creating this ama...</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Jun 16 00:21:22 +0000 2021</td>\n",
       "      <td>1405947462028873729</td>\n",
       "      <td>Former Pfizer VP and Virologist, Dr. Michael Y...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Jun 18 13:34:47 +0000 2021</td>\n",
       "      <td>1405947503225229317</td>\n",
       "      <td>I think itâ€™s important that we donâ€™t sell COVA...</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at             tweet_id  \\\n",
       "0  Thu Jun 17 16:18:28 +0000 2021  1405947374003015684   \n",
       "1  Fri Jun 18 16:40:24 +0000 2021  1405947412364075010   \n",
       "2  Fri Jun 18 17:45:27 +0000 2021  1405947447797587969   \n",
       "3  Wed Jun 16 00:21:22 +0000 2021  1405947462028873729   \n",
       "4  Fri Jun 18 13:34:47 +0000 2021  1405947503225229317   \n",
       "\n",
       "                                        cleaned_text lang possibly_sensitive  \n",
       "0  ðŸš¨Africa is \"in the midst of a full-blown third...   en              False  \n",
       "1  Dr Moeti is head of WHO in Africa, and one of ...   en              False  \n",
       "2  Thank you @research2note for creating this ama...   en               None  \n",
       "3  Former Pfizer VP and Virologist, Dr. Michael Y...   en              False  \n",
       "4  I think itâ€™s important that we donâ€™t sell COVA...   en               None  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TweetDfExtractor:\n",
    "    \"\"\"\n",
    "    this function will parse tweets json into a pandas dataframe\n",
    "    \n",
    "    Return\n",
    "    #------\n",
    "    dataframe\n",
    "    \"\"\"\n",
    "    def __init__(self, tweets_list):\n",
    "        \n",
    "        self.tweets_list = tweets_list\n",
    "\n",
    "        \n",
    "    def find_full_text(self)->list:\n",
    "        text = []\n",
    "        for element in self.tweets_list:\n",
    "            if 'retweeted_status' in element:\n",
    "                if 'extended_tweet' in element['retweeted_status']:\n",
    "                    text.append(element['retweeted_status']['extended_tweet']['full_text'])\n",
    "                else:\n",
    "                    text.append(element['retweeted_status']['text'])\n",
    "            else:\n",
    "                try:\n",
    "\n",
    "                    if 'extended_tweet' in element['quoted_status']:\n",
    "                        text.append(element['quoted_status']['extended_tweet']['full_text'])\n",
    "                    else:\n",
    "                        text.append(element['quoted_status']['text'])\n",
    "                except:\n",
    "                    text.append(element['text'])\n",
    "\n",
    "\n",
    "        return text\n",
    "    \n",
    "\n",
    "    def find_created_time(self)->list:\n",
    "        created_at = [] # Initialize empty list\n",
    "        for element in self.tweets_list:\n",
    "            if 'retweeted_status' in element:\n",
    "                created_at.append(element['retweeted_status']['created_at'])\n",
    "                    \n",
    "            else:\n",
    "                created_at.append(element['created_at'])\n",
    "\n",
    "        return created_at\n",
    "\n",
    "    \n",
    "\n",
    "    def is_sensitive(self)->list:\n",
    "        is_sensitive = []\n",
    "        for element in self.tweets_list:\n",
    "            if 'retweeted_status' in element:\n",
    "                try:\n",
    "                    is_sensitive.append(element['retweeted_status']['possibly_sensitive'])\n",
    "                except:\n",
    "                    is_sensit = None\n",
    "                    is_sensitive.append(is_sensit)\n",
    "            else:\n",
    "                is_sensit = None\n",
    "                is_sensitive.append(is_sensit)\n",
    "\n",
    "        return is_sensitive\n",
    "\n",
    "    \n",
    "\n",
    "    def find_lang(self)->list:\n",
    "        lang = []\n",
    "        for element in self.tweets_list:\n",
    "            if 'lang' in element:\n",
    "                lang.append(element['lang'])\n",
    "                    \n",
    "            else:\n",
    "                lang = None\n",
    "                \n",
    "        return lang\n",
    "    \n",
    "    def find_tweet_id(self)->list:\n",
    "        tweet_id = []\n",
    "        for element in self.tweets_list:\n",
    "            if 'lang' in element:\n",
    "                tweet_id.append(element['id'])\n",
    "                    \n",
    "            else:\n",
    "                tweet_id = None\n",
    "                \n",
    "        return tweet_id\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    def get_tweet_df(self, save=False)->pd.DataFrame:\n",
    "        \"\"\"required column to be generated you should be creative and add more features\"\"\"\n",
    "        \n",
    "        columns = ['created_at', 'tweet_id','cleaned_text', 'lang','possibly_sensitive']\n",
    "        \n",
    "        created_at = self.find_created_time()\n",
    "#         print(len(created_at))\n",
    "        text = self.find_full_text()\n",
    "#         print(len(text))\n",
    "        lang = self.find_lang()\n",
    "#         print(len(lang))\n",
    "        tweet_id = self.find_tweet_id()\n",
    "#         print(tweet_id[:50])\n",
    "        sensitivity = self.is_sensitive()\n",
    "#         print(len(sensitivity))\n",
    "        data = zip(created_at, tweet_id, text, lang, sensitivity)\n",
    "\n",
    "        df = pd.DataFrame(data=data, columns=columns)\n",
    "        if True:\n",
    "            df.to_csv('processed_tweet_data.csv', index=False)\n",
    "            print('File Successfully Saved.!!!')\n",
    "        \n",
    "        return df\n",
    "\n",
    "                \n",
    "\n",
    "# required column to be generated you should be creative and add more features\n",
    "columns = ['created_at', 'original_text','cleaned_text', 'sentiment','polarity','subjectivity', 'lang','possibly_sensitive']\n",
    "tweet_len, tweet_list = read_json(\"data/covid19.json\")\n",
    "tweet_list[:1]\n",
    "tweet = TweetDfExtractor(tweet_list)\n",
    "tweet_df = tweet.get_tweet_df() \n",
    "tweet_df.head()\n",
    "# use all defined functions to generate a dataframe with the specified columns above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "83130d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6532 entries, 0 to 6531\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   created_at          6532 non-null   object\n",
      " 1   tweet_id            6532 non-null   int64 \n",
      " 2   cleaned_text        6532 non-null   object\n",
      " 3   lang                6532 non-null   object\n",
      " 4   possibly_sensitive  3618 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 255.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#To get information abou the data\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "378c3c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6532 entries, 0 to 6531\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   created_at          6532 non-null   object\n",
      " 1   tweet_id            6532 non-null   int64 \n",
      " 2   cleaned_text        6532 non-null   object\n",
      " 3   lang                6532 non-null   object\n",
      " 4   possibly_sensitive  3618 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 255.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dropping Non-english texts\n",
    "indices = tweet_df.loc[tweet_df['lang']!='en'].index\n",
    "tweet_df.drop(indices, inplace=True)\n",
    "tweet_df=tweet_df.reset_index(drop=True)\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde53c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "47e41b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing value(s): 2914\n",
      "Columns having columns value:Index(['possibly_sensitive'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Determining None values in data\n",
    "\n",
    "print(\"The number of missing value(s): {}\".format(tweet_df.isnull().sum().sum()))\n",
    "print(\"Columns having columns value:{}\".format(tweet_df.columns[tweet_df.isnull().any()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d5b9039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanedTextProcessing:\n",
    "  def __init__(self,df):\n",
    "    self.df=df\n",
    "    \n",
    "  def preprocess_cleaned_text(self):\n",
    "    \n",
    "    #text Preprocessing\n",
    "    tweet_df['cleaned_text']=tweet_df['cleaned_text'].astype(str)\n",
    "    tweet_df['cleaned_text'] = tweet_df['cleaned_text'].apply(lambda x: x.lower())\n",
    "    tweet_df['cleaned_text']= tweet_df['cleaned_text'].apply(lambda x: x.translate(str.maketrans(' ', ' ', string.punctuation)))\n",
    "    \n",
    "    #Converting tweets to list of words For feature engineering\n",
    "    sentence_list = [tweet for tweet in tweet_df['cleaned_text']]\n",
    "    word_list = [sent.split() for sent in sentence_list]\n",
    "\n",
    "    #Create dictionary which contains Id and word \n",
    "    word_to_id = corpora.Dictionary(word_list)\n",
    "    corpus_1= [word_to_id.doc2bow(tweet) for tweet in word_list]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return word_list, word_to_id, corpus_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9d851028",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrepareData_obj=CleanedTextProcessing(tweet_df)\n",
    "word_list ,id2word,corpus=PrepareData_obj.preprocess_cleaned_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "faa2fea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ðŸš¨africa', 'is', 'in', 'the', 'midst', 'of', 'a', 'fullblown', 'third', 'wave', 'of', 'coronavirus', 'the', 'head', 'of', 'whoafro', 'has', 'warned', 'cases', 'have', 'risen', 'across', 'the', 'continent', 'by', 'more', 'than', '20', 'and', 'deaths', 'have', 'also', 'risen', 'by', '15', 'in', 'the', 'last', 'week', 'jriggers', 'reports', 'ðŸ§µ', 'httpstcocrdhqphfwm'], ['dr', 'moeti', 'is', 'head', 'of', 'who', 'in', 'africa', 'and', 'one', 'of', 'the', 'best', 'public', 'health', 'experts', 'and', 'leaders', 'i', 'know', 'hers', 'is', 'a', 'desperate', 'request', 'for', 'vaccines', 'to', 'africa', 'we', 'plead', 'with', 'germany', 'and', 'the', 'uk', 'to', 'lift', 'patent', 'restrictions', 'and', 'urgently', 'transfer', 'technology', 'to', 'enable', 'production', 'in', 'africa', 'httpstcosogiroihoc']]\n",
      "Dictionary(10341 unique tokens: ['15', '20', 'a', 'across', 'also']...)\n"
     ]
    }
   ],
   "source": [
    "# Results of Cleaned_text processing\n",
    "# print(corpus)\n",
    "print(word_list[:2])\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7689e70",
   "metadata": {},
   "source": [
    "## Topic Modeling Using Natural Language Processing\n",
    "\n",
    "### Model Used: Latent Dirichlet Allocation\n",
    "\n",
    "According to distributional hypothesis, (i.e. similar topics make use of similar words) and the statistical mixture hypothesis (i.e. documents talk about several topics) for which a statistical distribution can be determined. \n",
    "\n",
    "--> The purpose of LDA is mapping each teweets in our corpus to a set of topics which covers a good deal of the words in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e46b4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary packages\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0e814be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "53269b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('vaccines', 0.032263555),\n",
      "   ('age', 0.024298357),\n",
      "   ('capacity', 0.023284802),\n",
      "   ('dose1', 0.022089638),\n",
      "   ('min', 0.022089638),\n",
      "   ('pin', 0.022089638),\n",
      "   ('limit', 0.022089638),\n",
      "   ('covaxin', 0.018762687),\n",
      "   ('to', 0.017852047),\n",
      "   ('the', 0.015124568)]),\n",
      " (1,\n",
      "  [('the', 0.05904565),\n",
      "   ('of', 0.04670784),\n",
      "   ('india', 0.034666907),\n",
      "   ('in', 0.03077423),\n",
      "   ('africa', 0.023735823),\n",
      "   ('to', 0.021278383),\n",
      "   ('covid19', 0.019526204),\n",
      "   ('is', 0.017535241),\n",
      "   ('a', 0.016430384),\n",
      "   ('and', 0.015595473)]),\n",
      " (2,\n",
      "  [('the', 0.043014195),\n",
      "   ('to', 0.039461844),\n",
      "   ('covid19', 0.030880341),\n",
      "   ('vaccines', 0.023449393),\n",
      "   ('of', 0.022783972),\n",
      "   ('in', 0.018389795),\n",
      "   ('are', 0.018339787),\n",
      "   ('amp', 0.014665315),\n",
      "   ('and', 0.01390819),\n",
      "   ('with', 0.012017919)]),\n",
      " (3,\n",
      "  [('and', 0.037099864),\n",
      "   ('to', 0.032912906),\n",
      "   ('the', 0.023691649),\n",
      "   ('africa', 0.019669373),\n",
      "   ('in', 0.01809949),\n",
      "   ('vaccines', 0.017910507),\n",
      "   ('covid19', 0.017458482),\n",
      "   ('of', 0.016442796),\n",
      "   ('is', 0.014867638),\n",
      "   ('why', 0.014137238)]),\n",
      " (4,\n",
      "  [('you', 0.057914387),\n",
      "   ('to', 0.04941147),\n",
      "   ('vaccines', 0.048051994),\n",
      "   ('just', 0.0392367),\n",
      "   ('by', 0.03918694),\n",
      "   ('israel', 0.038929094),\n",
      "   ('your', 0.035807207),\n",
      "   ('expired', 0.033038188),\n",
      "   ('another', 0.031987388),\n",
      "   ('off', 0.0319633)])]\n"
     ]
    }
   ],
   "source": [
    "# Showing Topics\n",
    "pprint(lda_model.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ba3c1",
   "metadata": {},
   "source": [
    "* Each line is a topic with individual topic terms and weights. Topic0  can be termed as Vaccination describing the dosage, age and capacity, Topic2 can be termed as Covid19 Vaccines and Topic3 can be termed as Covid19 Vaccination in Africa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008c95f",
   "metadata": {},
   "source": [
    "# **Model Analysis**\n",
    "\n",
    "Perplexity is also a measure of model quality and in natural language processing is often used as â€œperplexity per number of wordsâ€. It describes how well a model predicts a sample, i.e. how much it is â€œperplexedâ€ by a sample from the observed data. The lower the score, the better the model for the given data.\n",
    "\n",
    "A coherence matrix is used to test the model for accuracy. Topic coherence is a measure that compares different topic models based on their human-interpretability. The coherence score â€˜C_Vâ€™ provides a numerical value to the interpretability of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4a11871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.532030837516905\n",
      "\n",
      " Ldamodel Coherence Score/Accuracy on Tweets Topic Modeling:  0.5050434234621697\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "\n",
    "#It's a measure of how good the model is. The lower the better. Perplexity is a negative value\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\n Ldamodel Coherence Score/Accuracy on Tweets Topic Modeling: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9f614",
   "metadata": {},
   "source": [
    "* > Basic Ldamodel Coherence Score 0.505, This means that the model has performed reasonably well in Tweet topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc17f6a",
   "metadata": {},
   "source": [
    "### Analyising How Topics relates to each Other using pyLDAvis Package\n",
    "Anlayizing results Exploring the Intertopic Distance Plot, helps to learn about how topics relate to each other, including potential higher-level structure between groups of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76203d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing pyLDAvis package\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70290093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
